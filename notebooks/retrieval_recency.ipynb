{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.040416Z",
     "start_time": "2025-05-02T12:36:19.037342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_queries = [\n",
    "    \"How do I connect to Outlook?\",\n",
    "    \"What are the onboarding process at DTSC?\",\n",
    "    \"How can I access my Officient Calendar with Outlook?\"\n",
    "    ]\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.974396Z",
     "start_time": "2025-05-02T12:36:19.103337Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from unbiasai.config import DATA_DIR\n",
    "from unbiasai.utils import generate_embedding, generate_response\n",
    "from unbiasai.utils import get_embedding\n",
    "#from deepseek import Embeddings"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_embedding' from 'unbiasai.utils' (/Users/marco/Work-MBP/UnBiasAI/src/unbiasai/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdotenv\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_dotenv\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01munbiasai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DATA_DIR\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01munbiasai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m generate_embedding, generate_response\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01munbiasai\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_embedding\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m#from deepseek import Embeddings\u001B[39;00m\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'generate_embedding' from 'unbiasai.utils' (/Users/marco/Work-MBP/UnBiasAI/src/unbiasai/utils.py)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.976286Z",
     "start_time": "2025-05-02T12:29:44.664300Z"
    }
   },
   "source": [
    "file_path = DATA_DIR / 'dataset_retrieval_recency.csv'\n",
    "df = pd.read_csv(file_path)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subset of data for test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.976391Z",
     "start_time": "2025-05-02T12:29:44.675868Z"
    }
   },
   "source": [
    "df_30 = df.head(30)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load API key"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.980859Z",
     "start_time": "2025-05-02T12:29:44.682735Z"
    }
   },
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "model_name = \"deepseek\"\n",
    "\n",
    "#api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "#model_name = \"cohere\"\n",
    "\n",
    "api_key = os.getenv(\"CLAUDE_API_KEY\")\n",
    "model_name = \"claude\"\n",
    "\n",
    "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#model_name = \"gpt\"\n",
    "\n",
    "text = 'this is a test'\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.981388Z",
     "start_time": "2025-05-02T12:29:44.690153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#initialize llm\n",
    "from unbiasai.utils import initialize_llm\n",
    "initialized_llm = initialize_llm(model_name, api_key)\n",
    "embeddings = generate_embedding(initialized_llm)\n",
    "response = generate_response(text, embeddings)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized correctly: claude, llm: model='claude-3-7-sonnet-latest' anthropic_api_url='https://api.anthropic.com' anthropic_api_key=SecretStr('**********') model_kwargs={}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:36:19.981867Z",
     "start_time": "2025-05-02T12:29:44.703560Z"
    }
   },
   "cell_type": "code",
   "source": "generate_embedding('Test test', initialized_llm, api_key)",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mgenerate_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mTest test\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitialized_llm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/src/unbiasai/utils.py:16\u001B[39m, in \u001B[36mgenerate_embedding\u001B[39m\u001B[34m(text, initialized_llm, api_key)\u001B[39m\n\u001B[32m     13\u001B[39m openai_api_key = os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,\n\u001B[32m     15\u001B[39m                               model=\u001B[33m\"\u001B[39m\u001B[33mtext-embedding-3-small\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m response = \u001B[43membeddings\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m embedding = response\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m embedding\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:617\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_query\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    608\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[32m    609\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001B[39;00m\n\u001B[32m    610\u001B[39m \n\u001B[32m    611\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    615\u001B[39m \u001B[33;03m        Embedding for the text.\u001B[39;00m\n\u001B[32m    616\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:576\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    574\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    575\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:471\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    469\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    470\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m471\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    475\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/openai/resources/embeddings.py:128\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    122\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    123\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    124\u001B[39m             ).tolist()\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/openai/_base_client.py:1239\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1225\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1226\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1227\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1234\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1235\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1236\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1237\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1238\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1239\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/openai/_base_client.py:1034\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1031\u001B[39m             err.response.read()\n\u001B[32m   1033\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1034\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1036\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mRateLimitError\u001B[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embedding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_30['embedding'] = df['content'].apply(generate_embedding, args=[initialized_llm])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert documents in supabase table, IMPORTANT: change function so supabase table name can be changed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "insert_documents(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Supabase"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Connect to Supabase\n",
    " ### Supabase Credentials\n",
    "SUPABASE_URL = \"https://rrjbrtbsvdoxndchvchq.supabase.co\"\n",
    "SUPABASE_KEY = supabase_key\n",
    "\n",
    "\n",
    "# Create Supabase client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve Documents for Each Query Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize a list to collect data\n",
    "\n",
    "models = [\"gpt\",  \"claude\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "# retrieval_results structure: { model_name: { query: [list of document results] } }\n",
    "retrieval_results = {model: {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    for query in test_queries:\n",
    "        # Set re_rank=True if you wish to re-rank documents using the LLM.\n",
    "        retrieval_results[model][query] = retrieve(query, model, k=4, re_rank=True)\n",
    "print(\"Retrieval complete for all models and queries.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
