{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTcOAFZGRv1a",
        "outputId": "72b29d29-bbdc-41d1-9c25-1c29a8b54815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/243.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m235.5/243.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m259.2/259.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai anthropic cohere scikit-learn scipy matplotlib pandas supabase requests langchain_openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1USPnn6YLil",
        "outputId": "4f79c19e-e217-49e7-8d33-a67d6cf8d47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'anthropic'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5c4c632233e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0manthropic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anthropic'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import openai\n",
        "import anthropic\n",
        "import cohere\n",
        "import requests\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.stats import f_oneway\n",
        "from supabase import create_client\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from langchain_openai import OpenAIEmbeddings  # New package as per deprecation warning\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Flkk7CSSYf04"
      },
      "outputs": [],
      "source": [
        "#REading the API key\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OpenAI')\n",
        "supabase_key = userdata.get('Supabase_key') #vector store\n",
        "MISTRAL_API_KEY = userdata.get('Mistral')\n",
        "CLAUDE_API_KEY = userdata.get('Anthropic')\n",
        "COHERE_API_KEY = userdata.get('Cohere')\n",
        "DEEPSEEK_API_KEY = userdata.get('Deepseek_new')\n",
        "GEMINI_API_KEY = userdata.get('Gemini')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1paIbe6ZWPn"
      },
      "outputs": [],
      "source": [
        " ### Supabase Credentials\n",
        "SUPABASE_URL = \"https://rrjbrtbsvdoxndchvchq.supabase.co\"\n",
        "SUPABASE_KEY = supabase_key\n",
        "\n",
        "# Initialize API clients\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "anthropic_client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
        "cohere_client = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "# Create Supabase client\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxQs-JxlaWDT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title âš™ï¸ Step 4: Configure Experiment\n",
        "# @markdown Select which models to include in your experiment:\n",
        "\n",
        "use_claude = True # @param {type:\"boolean\"}\n",
        "use_openai = True # @param {type:\"boolean\"}\n",
        "use_mistral = True # @param {type:\"boolean\"}\n",
        "use_cohere = True # @param {type:\"boolean\"}\n",
        "use_deepseek = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Define the test questions:\n",
        "question_text = \"What is the Cafeteria plan?\" # @param {type:\"string\"}\n",
        "question_list = [\n",
        "   \"What are the Cafeteria plan benefits?\", # 3989\n",
        "   \"How can I modify a page on dtsc.be?\", # 3999\n",
        "   \"How do I access the shared library?\", # 4001\n",
        "   \"Who is responsible if I receive a traffic fine?\", # 4008\n",
        "   \"How do I create a teams meeting?\", # 4017\n",
        "   \"What activities are included in the DTeam Spirit Challenge?\", # 4020\n",
        "   \"Can you tell me about Officient employee self-service?\", # 4026\n",
        "   \"Tell me about the Onboarding To Do List?\", # 4027\n",
        "   \"How can I activate music streaming on my company car?\", # 4043\n",
        "   \"How to access a shared mailbox?\" # 4047\n",
        "]\n",
        "\n",
        "num_iterations = 50 # @param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "# @markdown Advanced settings:\n",
        "top_k_docs = 2 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "temperature = 0.7 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "match_threshold = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Define LLM models\n",
        "llm_models = {}\n",
        "\n",
        "if use_claude:\n",
        "    llm_models[\"Claude\"] = {\"provider\": \"anthropic\", \"model\": \"claude-3-7-sonnet-latest\"}\n",
        "if use_openai:\n",
        "    llm_models[\"GPT\"] = {\"provider\": \"openai\", \"model\": \"gpt-4o\"}\n",
        "if use_mistral:\n",
        "    llm_models[\"Mistral\"] = {\"provider\": \"mistral\", \"model\": \"mistral-large-latest\"}\n",
        "if use_cohere:\n",
        "    llm_models[\"Cohere\"] = {\"provider\": \"cohere\", \"model\": \"command-a-03-2025\"}\n",
        "if use_deepseek:\n",
        "    llm_models[\"Deepseek\"] = {\"provider\": \"deepseek\", \"model\": \"deepseek-chat\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuXgVBd-i5gj"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"Get embeddings for a text using OpenAI's embedding model\"\"\"\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")  # Use the appropriate model\n",
        "    response = embeddings.embed_query(text)  # Correctly call the method to generate embeddings\n",
        "    embedding = response\n",
        "    return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NfVh6-danUX"
      },
      "outputs": [],
      "source": [
        "# Generate the list of test questions\n",
        "questions = question_list * num_iterations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "VADCUgDulluc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSx3balxa58K"
      },
      "outputs": [],
      "source": [
        "# @title ğŸ” Step 5: Vector Search Function\n",
        "def retrieve_context(query, top_k=top_k_docs):\n",
        "    \"\"\"Retrieve relevant documents from Supabase vector store using OpenAI embeddings and RPC with filter support\"\"\"\n",
        "    try:\n",
        "        # Get embedding for the query using OpenAI\n",
        "        query_embedding = get_embedding(query)\n",
        "\n",
        "        # Call the custom Supabase function via RPC with filter\n",
        "        response = supabase.rpc(\n",
        "            'match_documents',\n",
        "            {\n",
        "                'query_embedding': query_embedding,\n",
        "                'match_count': top_k,\n",
        "                'filter': {}  # You can pass filters here later if needed\n",
        "            }\n",
        "        ).execute()\n",
        "\n",
        "        if not response.data or len(response.data) == 0:\n",
        "            return \"No relevant context found in the knowledge base.\"\n",
        "\n",
        "        # Format retrieved documents as context\n",
        "        context = \"Context from knowledge base:\\n\\n\"\n",
        "        for i, doc in enumerate(response.data):\n",
        "            content = doc.get('content', '')\n",
        "            source = doc.get('source', 'Unknown source')\n",
        "            context += f\"Document {i+1}: {content}\\nSource: {source}\\n\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving from vector store: {e}\")\n",
        "        return \"Error retrieving context from knowledge base.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GSgcnbAkRm4"
      },
      "outputs": [],
      "source": [
        "#sanity check\n",
        "query = \"How can I access shared mailbox?\"\n",
        "context = retrieve_context(query)\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvgioOFvcVCe"
      },
      "outputs": [],
      "source": [
        "# @title ğŸ§  Step 6: Query LLM with RAG for Different Providers\n",
        "def query_llm_with_rag(llm_config, query):\n",
        "    provider = llm_config[\"provider\"]\n",
        "    model = llm_config[\"model\"]\n",
        "\n",
        "    # Retrieve relevant context from Supabase\n",
        "    context = retrieve_context(query)\n",
        "\n",
        "    # Construct RAG prompt with retrieved context\n",
        "    rag_prompt = f\"\"\"\n",
        "{context}\n",
        "\n",
        "Based on the above context from our knowledge base, please answer the following question:\n",
        "{query}\n",
        "\n",
        "If the context doesn't contain relevant information to answer the question,\n",
        "please say so and answer based on your general knowledge.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Call the appropriate API based on provider\n",
        "        if provider == \"openai\":\n",
        "            # Updated OpenAI client API call for v1.0+\n",
        "            response = openai.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "                temperature=temperature\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        elif provider == \"anthropic\":\n",
        "            response = anthropic_client.messages.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "                max_tokens=1000,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            return response.content[0].text\n",
        "\n",
        "        elif provider == \"cohere\":\n",
        "            response = cohere_client.chat(\n",
        "                message=rag_prompt,\n",
        "                model=model,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            return response.text\n",
        "\n",
        "        elif provider == \"mistral\":\n",
        "            headers = {\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"Accept\": \"application/json\",\n",
        "                \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\"\n",
        "            }\n",
        "            payload = {\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "                \"temperature\": temperature\n",
        "            }\n",
        "            response = requests.post(\n",
        "                \"https://api.mistral.ai/v1/chat/completions\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "        elif provider == \"deepseek\":\n",
        "            headers = {\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n",
        "            }\n",
        "            payload = {\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "                \"temperature\": temperature\n",
        "            }\n",
        "            response = requests.post(\n",
        "                \"https://api.deepseek.com/v1/chat/completions\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "        else:\n",
        "            return f\"Error: Unsupported provider {provider}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error with {provider} ({model}): {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity checks\n",
        "query = \"What is a cafeteria plan?\"\n",
        "response = query_llm_with_rag(llm_models[\"Deepseek\"], query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EOZf1apWDjf_",
        "outputId": "8e461130-1c6f-4f6f-bd8c-f0036cac09af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, a **Cafeteria Plan** is an employee benefits program that allows employees to customize their compensation package by choosing from a menu of benefits tailored to their individual needs and preferences.  \n",
            "\n",
            "### Key Features of the Cafeteria Plan at DTSC:  \n",
            "1. **Flexibility** â€“ Employees can select benefits that align with their personal and financial goals rather than a one-size-fits-all approach.  \n",
            "2. **Options Available** â€“ Choices include pension savings, IT device coverage, hospitalization insurance, extra vacation days, gym memberships, and more.  \n",
            "3. **Annual Selection Process** â€“ Around **November 25th**, employees receive a document listing options and have **three weeks** to submit their preferences.  \n",
            "4. **Advantages** â€“ The plan promotes empowerment, work-life balance, financial planning, and job satisfaction while aiding in talent retention.  \n",
            "\n",
            "This system is designed to give employees control over their benefits, ensuring they receive what matters most to them.  \n",
            "\n",
            "Would you like further details on any specific aspect of the plan?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhPPJXM8ccNX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title ğŸ“ˆ Step 7: Drift Calculation\n",
        "def compute_response_drift_score(responses):\n",
        "    if len(responses) < 2:\n",
        "        return 0, []\n",
        "\n",
        "    base_embedding = get_embedding.encode(responses[0])\n",
        "    scores = []\n",
        "    for r in responses[1:]:\n",
        "        emb = get_embedding.encode(r)\n",
        "        sim = cosine_similarity([base_embedding], [emb])[0][0]\n",
        "        drift_score = 1 - sim\n",
        "        scores.append(drift_score)\n",
        "    return np.mean(scores), scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_A-e6YScquI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment():\n",
        "    # Track unique questions and their repetitions\n",
        "    unique_questions = {}  # Maps question text to question base ID\n",
        "    question_counts = {}   # Tracks how many times each unique question has been asked\n",
        "\n",
        "    response_logs_by_question = {}\n",
        "    df_data = []\n",
        "\n",
        "    for i, question in enumerate(questions):\n",
        "        # Check if this is a repetition of a previous question\n",
        "        if question in unique_questions:\n",
        "            base_question_id = unique_questions[question]\n",
        "            question_counts[question] += 1\n",
        "            repetition_num = question_counts[question]\n",
        "            question_id = f\"{base_question_id}_{repetition_num}\"  # e.g., \"q1_2\" for second repetition\n",
        "        else:\n",
        "            # First time seeing this question\n",
        "            base_question_id = f\"q{len(unique_questions) + 1}\"\n",
        "            unique_questions[question] = base_question_id\n",
        "            question_counts[question] = 1\n",
        "            repetition_num = 1\n",
        "            question_id = f\"{base_question_id}_{repetition_num}\"  # e.g., \"q1_1\" for first instance\n",
        "\n",
        "        print(f\"\\n Iteration {i+1}/{len(questions)} â€” Question: {question} (ID: {question_id})\")\n",
        "\n",
        "        # Initialize the entry for this question instance\n",
        "        response_logs_by_question[question_id] = {\n",
        "            \"question_text\": question,\n",
        "            \"base_question_id\": base_question_id,\n",
        "            \"repetition_num\": repetition_num,\n",
        "            \"responses\": {llm: [] for llm in llm_models}\n",
        "        }\n",
        "\n",
        "        for llm_name, config in llm_models.items():\n",
        "            print(f\"\\n {llm_name} is generating a response...\")\n",
        "            try:\n",
        "                response = query_llm_with_rag(config, question)\n",
        "                #print(f\" {llm_name} says:\\n{response}\\n\")\n",
        "\n",
        "                # Store the response in the nested structure\n",
        "                response_logs_by_question[question_id][\"responses\"][llm_name].append(response)\n",
        "\n",
        "                # Add row to DataFrame data\n",
        "                df_data.append({\n",
        "                    \"question_id\": question_id,\n",
        "                    \"base_question_id\": base_question_id,\n",
        "                    \"repetition_num\": repetition_num,\n",
        "                    \"question_text\": question,\n",
        "                    \"llm\": llm_name,\n",
        "                    \"response\": response,\n",
        "                    \"iteration\": i+1\n",
        "                })\n",
        "\n",
        "                # Add a small delay to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Error with {llm_name}: {e}\")\n",
        "\n",
        "    # Create DataFrame from collected data\n",
        "    response_df = pd.DataFrame(df_data)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"llm_rag_drift_log_{timestamp}.csv\"\n",
        "    response_df.to_csv(filename, index=False)\n",
        "    print(f\"\\nâœ… Log saved to '{filename}'\")\n",
        "\n",
        "    # For Google Colab: Allow downloading the file\n",
        "    try:\n",
        "        files.download(filename)\n",
        "    except:\n",
        "        print(f\"To download the log file, use the Files panel on the left sidebar.\")\n",
        "\n",
        "    return response_logs_by_question, response_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "response_logs, response_df = run_experiment()"
      ],
      "metadata": {
        "id": "Bo0FGq7tHvSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u2vNxKtfMsXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZHtXPng-bPXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this code accomplishes:**\n",
        "\n",
        "This code helps evaluate how consistent different LLMs are over multiple responses. A lower drift score means the LLM is giving more consistent responses across iterations, while higher drift means the responses are changing more significantly.\n",
        "The ANOVA test determines whether the different LLMs have statistically different levels of consistency (drift). The visualization helps to see trends in drift over time and compare performance between models.\n",
        "\n",
        "**Function: compute_response_drift_score**\n",
        "\n",
        "This function measures how much consecutive responses from the same LLM differ from each other:\n",
        "\n",
        "It takes a list of responses as input\n",
        "If there are fewer than 2 responses, it returns 0 (no drift can be calculated)\n",
        "It converts all responses into embeddings (vector representations) using a function called get_embedding\n",
        "It calculates the cosine similarity between consecutive embeddings\n",
        "It converts similarity to drift by subtracting from 1 (so higher values mean more difference)\n",
        "It returns both the average drift and a list of all drift scores\n",
        "\n",
        "\n",
        "**Function: analyze_drift**\n",
        "\n",
        "This function analyzes drift across multiple LLMs:\n",
        "\n",
        "It processes each LLM's responses to calculate drift scores\n",
        "It checks if there are enough responses (at least 2) to calculate drift\n",
        "It performs ANOVA (Analysis of Variance) test to check if different LLMs have statistically significant differences in drift\n",
        "It prints results including average drift scores for each LLM and ANOVA results\n",
        "It creates and saves a plot showing drift over time for each LLM"
      ],
      "metadata": {
        "id": "4iNRBGyDbSvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIO2fXi_cxD0"
      },
      "outputs": [],
      "source": [
        "def analyze_drift_for_repeated_questions_df(response_df):\n",
        "    \"\"\"\n",
        "    Analyze drift across repeated instances of the same questions using DataFrame\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ“Š Calculating Response Drift Scores for Repeated Questions...\")\n",
        "\n",
        "    # Extract unique LLM models from df\n",
        "    llm_models = response_df['llm'].unique()\n",
        "\n",
        "    # Group question IDs by base question\n",
        "    questions_by_base = {}\n",
        "    for _, row in response_df.drop_duplicates(['question_id']).iterrows():\n",
        "        base_id = row['base_question_id']\n",
        "        q_id = row['question_id']\n",
        "        if base_id not in questions_by_base:\n",
        "            questions_by_base[base_id] = []\n",
        "        questions_by_base[base_id].append(q_id)\n",
        "\n",
        "    # Sort question IDs within each base group by repetition number\n",
        "    for base_id in questions_by_base:\n",
        "        questions_by_base[base_id].sort(key=lambda q_id: int(q_id.split('_')[1]))\n",
        "\n",
        "    # Initialize drift logs\n",
        "    drift_logs = {llm: {base_id: [] for base_id in questions_by_base} for llm in llm_models}\n",
        "    drift_df_data = []\n",
        "\n",
        "    # Collect all drift scores by LLM for ANOVA\n",
        "    all_drift_by_llm = {llm: [] for llm in llm_models}\n",
        "\n",
        "    for base_id, q_ids in questions_by_base.items():\n",
        "        if len(q_ids) < 2:\n",
        "            print(f\"Base question {base_id} doesn't have multiple repetitions, skipping drift analysis\")\n",
        "            continue\n",
        "\n",
        "        # Get question text from first instance\n",
        "        question_text = response_df[response_df['question_id'] == q_ids[0]]['question_text'].iloc[0]\n",
        "        print(f\"\\nAnalyzing drift for question: '{question_text[:50]}...' (Base ID: {base_id})\")\n",
        "\n",
        "        for llm in llm_models:\n",
        "            # Get all responses for this LLM across repetitions of this question\n",
        "            responses = []\n",
        "            for q_id in q_ids:\n",
        "                llm_responses = response_df[(response_df['question_id'] == q_id) & (response_df['llm'] == llm)]['response'].tolist()\n",
        "                if llm_responses:\n",
        "                    responses.append(llm_responses[0])\n",
        "\n",
        "            if len(responses) < 2:\n",
        "                print(f\"  {llm}: Not enough responses across repetitions to calculate drift\")\n",
        "                continue\n",
        "\n",
        "            # Calculate drift between consecutive repetitions\n",
        "            avg_drift, all_drifts = compute_response_drift_score(responses)\n",
        "            drift_logs[llm][base_id] = all_drifts\n",
        "\n",
        "            # Collect drift scores for ANOVA\n",
        "            all_drift_by_llm[llm].extend(all_drifts)\n",
        "\n",
        "            print(f\"  {llm} - Drift between repetitions: {', '.join([f'{d:.4f}' for d in all_drifts])}\")\n",
        "            print(f\"  {llm} - Average drift: {avg_drift:.4f}\")\n",
        "\n",
        "            # Add to DataFrame data\n",
        "            for i, drift_score in enumerate(all_drifts):\n",
        "                rep_from = i + 1\n",
        "                rep_to = i + 2\n",
        "                drift_df_data.append({\n",
        "                    \"base_question_id\": base_id,\n",
        "                    \"question_text\": question_text,\n",
        "                    \"llm\": llm,\n",
        "                    \"from_repetition\": rep_from,\n",
        "                    \"to_repetition\": rep_to,\n",
        "                    \"drift_score\": drift_score,\n",
        "                })\n",
        "\n",
        "    # Perform ANOVA test to compare LLMs\n",
        "    print(\"\\nğŸ“Š Statistical Analysis (ANOVA):\")\n",
        "    valid_llms = [llm for llm in llm_models if len(all_drift_by_llm[llm]) > 0]\n",
        "\n",
        "    if len(valid_llms) > 1:\n",
        "        # Prepare data for ANOVA test\n",
        "        anova_data = [all_drift_by_llm[llm] for llm in valid_llms]\n",
        "\n",
        "        # Make sure all groups have data\n",
        "        if all(len(data) > 0 for data in anova_data):\n",
        "            try:\n",
        "                from scipy.stats import f_oneway\n",
        "                anova_result = f_oneway(*anova_data)\n",
        "\n",
        "                print(f\"ANOVA F-statistic: {anova_result.statistic:.4f}, p-value: {anova_result.pvalue:.4f}\")\n",
        "\n",
        "                if anova_result.pvalue < 0.05:\n",
        "                    print(\"ğŸ¯ Statistically significant difference in drift detected between LLMs!\")\n",
        "\n",
        "                    # If significant, perform post-hoc tests to see which models differ\n",
        "                    try:\n",
        "                        from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "                        import numpy as np\n",
        "\n",
        "                        # Prepare data for Tukey's test\n",
        "                        all_values = []\n",
        "                        model_labels = []\n",
        "\n",
        "                        for i, llm in enumerate(valid_llms):\n",
        "                            all_values.extend(all_drift_by_llm[llm])\n",
        "                            model_labels.extend([llm] * len(all_drift_by_llm[llm]))\n",
        "\n",
        "                        # Perform Tukey's HSD test\n",
        "                        tukey_result = pairwise_tukeyhsd(np.array(all_values), model_labels, alpha=0.05)\n",
        "                        print(\"\\nTukey's HSD Post-hoc Test:\")\n",
        "                        print(tukey_result)\n",
        "                    except ImportError:\n",
        "                        print(\"Note: statsmodels not available for post-hoc tests\")\n",
        "                else:\n",
        "                    print(\"âœ… No significant difference in drift detected between LLMs\")\n",
        "            except ImportError:\n",
        "                print(\"Note: scipy.stats not available for ANOVA test\")\n",
        "        else:\n",
        "            print(\"Not enough data points for all LLMs to perform ANOVA test\")\n",
        "    else:\n",
        "        print(\"Need at least two LLMs with data to perform statistical comparison\")\n",
        "\n",
        "    # Create DataFrame for drift analysis\n",
        "    if drift_df_data:\n",
        "        drift_df = pd.DataFrame(drift_df_data)\n",
        "        drift_df.to_csv(\"drift_analysis.csv\", index=False)\n",
        "\n",
        "        # Create visualizations\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # For each base question, create a subplot\n",
        "        num_base_questions = len(questions_by_base)\n",
        "        cols = min(2, num_base_questions)\n",
        "        rows = (num_base_questions + cols - 1) // cols  # Ceiling division\n",
        "\n",
        "        for i, (base_id, _) in enumerate(questions_by_base.items()):\n",
        "            if len(questions_by_base[base_id]) < 2:\n",
        "                continue\n",
        "\n",
        "            plt.subplot(rows, cols, i+1)\n",
        "\n",
        "            for llm in llm_models:\n",
        "                if base_id in drift_logs[llm] and drift_logs[llm][base_id]:\n",
        "                    x = range(1, len(drift_logs[llm][base_id])+1)\n",
        "                    plt.plot(x, drift_logs[llm][base_id], marker='o', label=llm)\n",
        "\n",
        "            question_text = response_df[response_df['base_question_id'] == base_id]['question_text'].iloc[0]\n",
        "            plt.title(f\"Q{base_id[-1]}: {question_text[:30]}...\")\n",
        "            plt.xlabel(\"Transition Between Repetitions\")\n",
        "            plt.ylabel(\"Drift Score\")\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"repeated_questions_drift.png\")\n",
        "        plt.show()\n",
        "\n",
        "        # Summary bar chart of average drift by question and LLM\n",
        "        avg_drift_data = []\n",
        "        for llm in llm_models:\n",
        "            for base_id in questions_by_base:\n",
        "                if base_id in drift_logs[llm] and drift_logs[llm][base_id]:\n",
        "                    avg_drift = sum(drift_logs[llm][base_id]) / len(drift_logs[llm][base_id])\n",
        "                    question_text = response_df[response_df['base_question_id'] == base_id]['question_text'].iloc[0]\n",
        "                    avg_drift_data.append({\n",
        "                        \"base_question_id\": base_id,\n",
        "                        \"question_short\": question_text[:20] + \"...\",\n",
        "                        \"llm\": llm,\n",
        "                        \"avg_drift\": avg_drift\n",
        "                    })\n",
        "\n",
        "        if avg_drift_data:\n",
        "            avg_drift_df = pd.DataFrame(avg_drift_data)\n",
        "\n",
        "            plt.figure(figsize=(14, 6))\n",
        "            sns.barplot(x=\"base_question_id\", y=\"avg_drift\", hue=\"llm\", data=avg_drift_df)\n",
        "            plt.title(\"Average Drift by Question and LLM\")\n",
        "            plt.xlabel(\"Question\")\n",
        "            plt.ylabel(\"Average Drift Score\")\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"avg_drift_by_question.png\")\n",
        "            plt.show()\n",
        "\n",
        "    return drift_logs, drift_df if drift_df_data else None\n",
        "\n",
        "\n",
        "def compute_response_drift_score(responses):\n",
        "    \"\"\"\n",
        "    Calculate drift between consecutive responses\n",
        "\n",
        "    Parameters:\n",
        "    - responses: List of response texts ordered by repetition\n",
        "\n",
        "    Returns:\n",
        "    - avg_drift: Average drift score across all transitions\n",
        "    - drift_scores: List of individual drift scores between consecutive responses\n",
        "    \"\"\"\n",
        "    if len(responses) < 2:\n",
        "        return 0, []\n",
        "\n",
        "    # You'll need to implement your drift calculation logic here\n",
        "    # For example, using cosine similarity between embeddings\n",
        "\n",
        "    drift_scores = []\n",
        "    for i in range(len(responses) - 1):\n",
        "        # Calculate similarity between consecutive responses\n",
        "        # Here's a placeholder - replace with your actual similarity calculation\n",
        "        try:\n",
        "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "            from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "            # Using TF-IDF and cosine similarity as a measure of drift\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            tfidf_matrix = vectorizer.fit_transform([responses[i], responses[i+1]])\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "            # Convert similarity to drift (1 - similarity)\n",
        "            drift = 1 - similarity\n",
        "            drift_scores.append(drift)\n",
        "        except ImportError:\n",
        "            # Fallback to a simple text similarity if sklearn not available\n",
        "            import difflib\n",
        "            similarity = difflib.SequenceMatcher(None, responses[i], responses[i+1]).ratio()\n",
        "            drift = 1 - similarity\n",
        "            drift_scores.append(drift)\n",
        "\n",
        "    avg_drift = sum(drift_scores) / len(drift_scores) if drift_scores else 0\n",
        "    return avg_drift, drift_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_df = pd.read_csv(\"/content/llm_rag_drift_log_20250410_015221.csv\")"
      ],
      "metadata": {
        "id": "VXX-iuAQ5ldr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "drift_logs, drift_df = analyze_drift_for_repeated_questions_df(response_df)\n"
      ],
      "metadata": {
        "id": "b7z9XRAOJ7Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drift_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "kzpqMOV4SqAS",
        "outputId": "92f9c293-98d7-4bb9-cd31-6a6d2328b3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   base_question_id                          question_text       llm  \\\n",
              "0                q1  What are the Cafeteria plan benefits?    Claude   \n",
              "1                q1  What are the Cafeteria plan benefits?    Claude   \n",
              "2                q1  What are the Cafeteria plan benefits?       GPT   \n",
              "3                q1  What are the Cafeteria plan benefits?       GPT   \n",
              "4                q1  What are the Cafeteria plan benefits?   Mistral   \n",
              "5                q1  What are the Cafeteria plan benefits?   Mistral   \n",
              "6                q1  What are the Cafeteria plan benefits?    Cohere   \n",
              "7                q1  What are the Cafeteria plan benefits?    Cohere   \n",
              "8                q1  What are the Cafeteria plan benefits?  Deepseek   \n",
              "9                q1  What are the Cafeteria plan benefits?  Deepseek   \n",
              "10               q2    How can I modify a page on dtsc.be?    Claude   \n",
              "11               q2    How can I modify a page on dtsc.be?    Claude   \n",
              "12               q2    How can I modify a page on dtsc.be?       GPT   \n",
              "13               q2    How can I modify a page on dtsc.be?       GPT   \n",
              "14               q2    How can I modify a page on dtsc.be?   Mistral   \n",
              "15               q2    How can I modify a page on dtsc.be?   Mistral   \n",
              "16               q2    How can I modify a page on dtsc.be?    Cohere   \n",
              "17               q2    How can I modify a page on dtsc.be?    Cohere   \n",
              "18               q2    How can I modify a page on dtsc.be?  Deepseek   \n",
              "19               q2    How can I modify a page on dtsc.be?  Deepseek   \n",
              "\n",
              "    from_repetition  to_repetition  drift_score  \n",
              "0                 1              2     0.089315  \n",
              "1                 2              3     0.064863  \n",
              "2                 1              2     0.066217  \n",
              "3                 2              3     0.028247  \n",
              "4                 1              2     0.067029  \n",
              "5                 2              3     0.053908  \n",
              "6                 1              2     0.023514  \n",
              "7                 2              3     0.026278  \n",
              "8                 1              2     0.081906  \n",
              "9                 2              3     0.081304  \n",
              "10                1              2     0.045999  \n",
              "11                2              3     0.086368  \n",
              "12                1              2     0.075362  \n",
              "13                2              3     0.064949  \n",
              "14                1              2     0.039161  \n",
              "15                2              3     0.133148  \n",
              "16                1              2     0.047212  \n",
              "17                2              3     0.076961  \n",
              "18                1              2     0.115759  \n",
              "19                2              3     0.073421  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4df8121-5295-4e5e-bcb8-94f49f1cb59d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>base_question_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>llm</th>\n",
              "      <th>from_repetition</th>\n",
              "      <th>to_repetition</th>\n",
              "      <th>drift_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Claude</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.089315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Claude</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.064863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>GPT</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.066217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>GPT</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.028247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Mistral</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.067029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Mistral</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.053908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Cohere</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.023514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Cohere</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.026278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Deepseek</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.081906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>q1</td>\n",
              "      <td>What are the Cafeteria plan benefits?</td>\n",
              "      <td>Deepseek</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.081304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Claude</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.045999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Claude</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.086368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>GPT</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.075362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>GPT</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.064949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Mistral</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.039161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Mistral</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Cohere</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.047212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Cohere</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.076961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Deepseek</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.115759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>q2</td>\n",
              "      <td>How can I modify a page on dtsc.be?</td>\n",
              "      <td>Deepseek</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.073421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4df8121-5295-4e5e-bcb8-94f49f1cb59d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4df8121-5295-4e5e-bcb8-94f49f1cb59d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4df8121-5295-4e5e-bcb8-94f49f1cb59d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3fa04e71-884f-405a-be63-2fde2a7503a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fa04e71-884f-405a-be63-2fde2a7503a5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3fa04e71-884f-405a-be63-2fde2a7503a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5347eaea-97f9-40ef-868b-efb632e6eaf0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('drift_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5347eaea-97f9-40ef-868b-efb632e6eaf0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('drift_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "drift_df",
              "summary": "{\n  \"name\": \"drift_df\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"base_question_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"q2\",\n          \"q1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"How can I modify a page on dtsc.be?\",\n          \"What are the Cafeteria plan benefits?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"llm\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"GPT\",\n          \"Deepseek\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"from_repetition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"to_repetition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drift_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02819352842299723,\n        \"min\": 0.0235144333082401,\n        \"max\": 0.13314778347222322,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.08931454919587756,\n          0.07696132940716116\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9u8VNYHc1V3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title ğŸ“Š Step 10: Feedback Visualization\n",
        "def visualize_feedback(feedback_logs):\n",
        "    # Convert feedback to numeric values\n",
        "    feedback_values = {\"positive\": 1, \"neutral\": 0, \"negative\": -1, \"error\": None}\n",
        "\n",
        "    feedback_data = {}\n",
        "    for llm, feedback in feedback_logs.items():\n",
        "        feedback_data[llm] = [feedback_values.get(f) for f in feedback if feedback_values.get(f) is not None]\n",
        "\n",
        "    # Plot average feedback scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    avg_scores = {llm: np.mean(scores) if scores else 0 for llm, scores in feedback_data.items()}\n",
        "\n",
        "    models = list(avg_scores.keys())\n",
        "    scores = list(avg_scores.values())\n",
        "\n",
        "    # Sort by score\n",
        "    sorted_indices = np.argsort(scores)\n",
        "    sorted_models = [models[i] for i in sorted_indices]\n",
        "    sorted_scores = [scores[i] for i in sorted_indices]\n",
        "\n",
        "    bars = plt.barh(sorted_models, sorted_scores, color=['red' if s < 0 else 'green' if s > 0 else 'gray' for s in sorted_scores])\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    plt.xlabel(\"Average Feedback Score (-1: Negative, 0: Neutral, 1: Positive)\")\n",
        "    plt.title(\"Average User Feedback by LLM\")\n",
        "    plt.xlim(-1.1, 1.1)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"feedback_analysis.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "visualize_feedback(feedback_logs)"
      ],
      "metadata": {
        "id": "Cqxcc4IdLbrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO2Uu2OEc4GC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title ğŸ’¾ Step 11: Export Logs\n",
        "def export_logs(log_df):\n",
        "    df = pd.DataFrame(log_df)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"llm_rag_drift_log_{timestamp}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nâœ… Log saved to '{filename}'\")\n",
        "\n",
        "    # For Google Colab: Allow downloading the file\n",
        "    try:\n",
        "        files.download(filename)\n",
        "    except:\n",
        "        print(f\"To download the log file, use the Files panel on the left sidebar.\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "df_log = export_logs(log_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "7vMg2HUrLh8u",
        "outputId": "09231393-4676-4d59-eb05-babad9f4fa03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Log saved to 'llm_rag_drift_log_20250408_113449.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7bcc7555-f047-409d-b7c7-18b7c26ff17d\", \"llm_rag_drift_log_20250408_113449.csv\", 26192)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdRkx4pRdF9P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title ğŸ“ Step 12: Generate Summary Report\n",
        "def generate_report(df, drift_logs):\n",
        "    print(\"\\nğŸ“‹ Generating Summary Report...\")\n",
        "\n",
        "    # Calculate statistics per model\n",
        "    model_stats = {}\n",
        "    for llm in llm_models:\n",
        "       # Calculate average drift\n",
        "        avg_drift = np.mean(drift_logs[llm]) if len(drift_logs[llm]) > 0 else 0\n",
        "\n",
        "        model_stats[llm] = {\n",
        "            'provider': llm_models[llm]['provider'],\n",
        "            'model': llm_models[llm]['model'],\n",
        "            'avg_drift': avg_drift\n",
        "        }\n",
        "\n",
        "    # Create summary dataframe\n",
        "    summary_df = pd.DataFrame.from_dict(model_stats, orient='index').reset_index()\n",
        "    summary_df = summary_df.rename(columns={'index': 'llm'})\n",
        "\n",
        "    # Sort by feedback score (descending)\n",
        "    summary_df = summary_df.sort_values('feedback_score', ascending=False)\n",
        "\n",
        "    # Save summary\n",
        "    summary_filename = \"llm_performance_summary.csv\"\n",
        "    summary_df.to_csv(summary_filename, index=False)\n",
        "    print(f\"âœ… Summary report saved to '{summary_filename}'\")\n",
        "\n",
        "    # For Google Colab: Allow downloading the file\n",
        "    try:\n",
        "        files.download(summary_filename)\n",
        "    except:\n",
        "        print(f\"To download the summary file, use the Files panel on the left sidebar.\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nğŸ“Š LLM Performance Summary:\")\n",
        "    display(summary_df[['llm', 'avg_drift']])\n",
        "\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity checks\n",
        "summary_df = generate_report(df_log, drift_logs)"
      ],
      "metadata": {
        "id": "3xZDRKrOLyF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPJ-zNlR4cx"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"ğŸ”„ Starting Multi-LLM RAG Drift Analysis Experiment\")\n",
        "\n",
        "    # Check if API keys are provided\n",
        "    missing_keys = []\n",
        "    if use_claude and not CLAUDE_API_KEY: missing_keys.append(\"Anthropic (Claude)\")\n",
        "    if use_openai and not OPENAI_API_KEY: missing_keys.append(\"OpenAI\")\n",
        "    if use_mistral and not MISTRAL_API_KEY: missing_keys.append(\"Mistral\")\n",
        "    if use_cohere and not COHERE_API_KEY: missing_keys.append(\"Cohere\")\n",
        "    if use_deepseek and not DEEPSEEK_API_KEY: missing_keys.append(\"DeepSeek\")\n",
        "\n",
        "    if missing_keys:\n",
        "        print(f\"âŒ Missing API keys for: {', '.join(missing_keys)}\")\n",
        "        print(\"Please provide the required API keys and run again.\")\n",
        "        return\n",
        "\n",
        "    # Check if any models are selected\n",
        "    if not llm_models:\n",
        "        print(\"âŒ No LLM models selected. Please select at least one model.\")\n",
        "        return\n",
        "\n",
        "    # Check if Supabase connection works\n",
        "    if not SUPABASE_URL or not SUPABASE_KEY:\n",
        "        print(\"âŒ Missing Supabase credentials. Please provide URL and key.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        test_query = \"test connection\"\n",
        "        test_context = retrieve_context(test_query, top_k=1)\n",
        "        print(\"âœ… Supabase connection successful\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Supabase connection failed: {e}\")\n",
        "        print(\"Please check your Supabase credentials and try again.\")\n",
        "        return\n",
        "\n",
        "    # Print active models\n",
        "    print(\"\\nğŸ¤– Active models for this experiment:\")\n",
        "    for llm, config in llm_models.items():\n",
        "        print(f\"  - {llm} ({config['provider']}: {config['model']})\")\n",
        "\n",
        "    # Run the experiment\n",
        "    start_time = time.time()\n",
        "    response_logs = run_experiment()\n",
        "    data = pd.DataFrame(response_logs)\n",
        "    data.to_csv(\"response_logs.csv\", index=False)\n",
        "    drift_logs = analyze_drift(response_logs)\n",
        "    summary_df = generate_report(drift_logs)\n",
        "\n",
        "    # Calculate and display execution time\n",
        "    execution_time = time.time() - start_time\n",
        "    print(f\"\\nâœ… Experiment completed in {execution_time:.2f} seconds\")\n",
        "\n",
        "    return summary_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "9wu4PxJLe3cV",
        "collapsed": true,
        "outputId": "cf4c2a23-7d08-4b9e-b6e9-18145b30c30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Starting Multi-LLM RAG Drift Analysis Experiment\n",
            "âœ… Supabase connection successful\n",
            "\n",
            "ğŸ¤– Active models for this experiment:\n",
            "  - Claude (anthropic: claude-3-7-sonnet-latest)\n",
            "  - GPT (openai: gpt-4o-mini)\n",
            "  - Mistral (mistral: mistral-large-latest)\n",
            "  - Cohere (cohere: command-r-plus)\n",
            "  - Deepseek (deepseek: deepseek-chat)\n",
            "\n",
            "ğŸ“Œ Iteration 1/5 â€” Question: What is the Cafeteria plan?\n",
            "\n",
            "ğŸ¤– Claude is generating a response...\n",
            "ğŸ—¨ï¸ Claude says:\n",
            "Based on the provided context, the Cafeteria Plan is part of the employee benefits program at DT Services and Consulting. It's designed to allow employees to tailor their benefits package according to their individual needs and preferences, rather than having a one-size-fits-all approach.\n",
            "\n",
            "The plan enables employees to choose from a menu of different benefits, putting them \"in the driver's seat\" of their compensation package. Employees receive a document around November 25th each year with options to choose from, and they have approximately three weeks to complete and submit their selections.\n",
            "\n",
            "The Cafeteria Plan offers various benefits to choose from, including pension savings, IT devices and internet expenses coverage, hospitalization insurance, medical check-ups, company bicycle, extra vacation days, professional training, gym membership, home office furniture, smartwatch, and cash bonus options.\n",
            "\n",
            "The plan aims to provide empowerment, customization, job satisfaction, work-life balance, and financial benefits to employees.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <div style=\"padding: 10px; border-radius: 5px;\">\n",
              "                <p>What's your feedback on Claude's response?</p>\n",
              "                <button id=\"pos_btn_0_Claude\" style=\"background-color: #4CAF50; color: white; border: none; padding: 10px 20px; margin: 5px; cursor: pointer;\"\n",
              "                onclick=\"\n",
              "                    document.getElementById('feedback_value_0_Claude').value = 'positive';\n",
              "                    document.getElementById('feedback_form_0_Claude').submit();\n",
              "                \">Positive</button>\n",
              "\n",
              "                <button id=\"neu_btn_0_Claude\" style=\"background-color: #FFC107; color: white; border: none; padding: 10px 20px; margin: 5px; cursor: pointer;\"\n",
              "                onclick=\"\n",
              "                    document.getElementById('feedback_value_0_Claude').value = 'neutral';\n",
              "                    document.getElementById('feedback_form_0_Claude').submit();\n",
              "                \">Neutral</button>\n",
              "\n",
              "                <button id=\"neg_btn_0_Claude\" style=\"background-color: #F44336; color: white; border: none; padding: 10px 20px; margin: 5px; cursor: pointer;\"\n",
              "                onclick=\"\n",
              "                    document.getElementById('feedback_value_0_Claude').value = 'negative';\n",
              "                    document.getElementById('feedback_form_0_Claude').submit();\n",
              "                \">Negative</button>\n",
              "\n",
              "                <form id=\"feedback_form_0_Claude\" style=\"display:none\">\n",
              "                    <input id=\"feedback_value_0_Claude\" name=\"feedback\">\n",
              "                </form>\n",
              "                </div>\n",
              "                "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1ed7aeb8cd19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute the main function if this script is run directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-052763674253>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresponse_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Export raw logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-4b8c380fe979>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Get feedback using standard input as fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ‘ğŸ‘ Type feedback for {llm} (positive/neutral/negative): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mfeedback\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neutral\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"negative\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter 'positive', 'neutral', or 'negative': \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Execute the main function if this script is run directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}