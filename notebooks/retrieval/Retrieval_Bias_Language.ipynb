{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtrHliXj0Kh2",
        "outputId": "969e457a-fc4c-4158-c13e-23f071a9a56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain langchain-anthropic langchain-community faiss-cpu pypdf python-dotenv langchain_mistralai langchain_deepseek langchain_cohere asyncio psutil GPUtil supabase langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "from math import log\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import re\n",
        "import supabase\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import SupabaseVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings  # New package as per deprecation warning\n",
        "from langchain.vectorstores import SupabaseVectorStore\n",
        "from langchain.chat_models import ChatOpenAI  # For GPT-4 and as a placeholder for Mistral\n",
        "from langchain.chat_models import ChatAnthropic  # For Claude (ensure compatibility with your LangChain version)\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "import re\n",
        "from datetime import datetime\n",
        "from supabase import create_client, Client\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langdetect import detect\n",
        "import json\n"
      ],
      "metadata": {
        "id": "Ei4BsL540-5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REading the API key\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OpenAI')\n",
        "supabase_key = userdata.get('Supabase_key') #vector store\n",
        "MISTRAL_API_KEY = userdata.get('Mistral')\n",
        "CLAUDE_API_KEY = userdata.get('Anthropic')\n",
        "COHERE_API_KEY = userdata.get('Cohere')\n",
        "DEEPSEEK_API_KEY = userdata.get('Deepseek_new')\n",
        "GEMINI_API_KEY = userdata.get('Gemini')"
      ],
      "metadata": {
        "id": "9gy1Xykz0-73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to Supabase\n",
        " ### Supabase Credentials\n",
        "SUPABASE_URL = \"\"\n",
        "SUPABASE_KEY = supabase_key\n",
        "\n",
        "\n",
        "# Create Supabase client\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)"
      ],
      "metadata": {
        "id": "D33MIxbF0--o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"Get embeddings for a text using OpenAI's embedding model\"\"\"\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY,model=\"text-embedding-3-large\")  # Use the appropriate model\n",
        "    response = embeddings.embed_query(text)  # Correctly call the method to generate embeddings\n",
        "    embedding = response\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "GfkPANOR0_A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, model_name, k=10, re_rank=False):\n",
        "    \"\"\"\n",
        "    Retrieve top-k documents for a query using Supabase vector search with optional LLM re-ranking.\n",
        "\n",
        "    Parameters:\n",
        "      query (str): The search query.\n",
        "      model_name (str): One of 'gpt-4o-mini', 'claude', 'mistral', etc.\n",
        "      k (int): Number of top documents to retrieve.\n",
        "      re_rank (bool): Whether to re-rank the documents using the LLM.\n",
        "\n",
        "    Returns:\n",
        "      List[dict]: A list of dictionaries with document 'id', 'rank', and 'content'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize LLM\n",
        "    model_name = model_name.lower()\n",
        "    if model_name == \"gpt-4o\":\n",
        "        llm = ChatOpenAI(model_name=\"gpt-4o-2024-11-20\", openai_api_key=OPENAI_API_KEY)\n",
        "    elif model_name == \"claude\":\n",
        "        llm = ChatAnthropic(model=\"claude-3-7-sonnet-latest\", anthropic_api_key=CLAUDE_API_KEY)\n",
        "    elif model_name == \"mistral\":\n",
        "        llm = ChatMistralAI(model=\"mistral-large-latest\", mistral_api_key=MISTRAL_API_KEY)\n",
        "    elif model_name == \"cohere\":\n",
        "        llm = ChatCohere(model=\"command-a-03-2025\", cohere_api_key=COHERE_API_KEY)\n",
        "    elif model_name == \"deepseek\":\n",
        "        import os\n",
        "        os.environ[\"DEEPSEEK_API_KEY\"] = DEEPSEEK_API_KEY\n",
        "        llm = ChatDeepSeek(model=\"deepseek-v3-chat\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Get embedding\n",
        "        query_embedding = get_embedding(query)\n",
        "\n",
        "        # Step 2: Retrieve documents from Supabase via RPC\n",
        "        response = supabase.rpc(\n",
        "            'match_documents_language_no_filter',\n",
        "            {\n",
        "                'query_embedding': query_embedding,\n",
        "                'match_count': k\n",
        "            }\n",
        "        ).execute()\n",
        "\n",
        "        if not response.data or len(response.data) == 0:\n",
        "            print(\"No relevant documents found.\")\n",
        "            return []\n",
        "\n",
        "        # Step 3: Build document list\n",
        "        docs = [\n",
        "            type(\"Doc\", (object,), {\n",
        "                \"id\": doc.get(\"id\"),\n",
        "                \"page_content\": doc.get(\"content\", \"\"),\n",
        "                \"metadata\": doc.get(\"metadata\", {})\n",
        "            })()\n",
        "            for doc in response.data\n",
        "        ]\n",
        "\n",
        "        actual_k = min(k, len(docs))\n",
        "\n",
        "        # Step 4: Optional re-ranking\n",
        "        if re_rank and actual_k > 1:\n",
        "            try:\n",
        "                documents_text = \"\\n\\n\".join([\n",
        "                    f\"Document {i+1} (ID: {docs[i].id}):\\n{docs[i].page_content}\"\n",
        "                    for i in range(actual_k)\n",
        "                ])\n",
        "\n",
        "                prompt = f\"\"\"\n",
        "                Query: {query}\n",
        "\n",
        "                You will be given {actual_k} documents retrieved via semantic search.\n",
        "                Your task is to re-rank these documents in order of their relevance to the query.\n",
        "                Please return EXACTLY {actual_k} document numbers in order, from MOST to LEAST relevant,\n",
        "                separated by commas (e.g., \"3,1,2\").\n",
        "\n",
        "                Documents:\n",
        "                {documents_text}\n",
        "                \"\"\"\n",
        "\n",
        "                messages = [\n",
        "                    SystemMessage(content=\"You are a helpful assistant skilled at ranking document relevance.\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "\n",
        "                llm_response = llm.invoke(messages)\n",
        "                ranking_text = llm_response.content.strip()\n",
        "                ranking_order = [int(num.strip()) - 1 for num in re.findall(r'\\d+', ranking_text)]\n",
        "\n",
        "                if len(ranking_order) != actual_k or sorted(ranking_order) != list(range(actual_k)):\n",
        "                    print(f\"Invalid ranking received: {ranking_text}. Using default order.\")\n",
        "                    ranking_order = list(range(actual_k))\n",
        "\n",
        "                docs = [docs[i] for i in ranking_order]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Re-ranking failed: {e}. Using initial ranking.\")\n",
        "\n",
        "        # Step 5: Return formatted result\n",
        "        results = [\n",
        "            {\n",
        "                \"id\": doc.id,\n",
        "                \"rank\": idx + 1,\n",
        "                \"content\": doc.page_content\n",
        "            }\n",
        "            for idx, doc in enumerate(docs)\n",
        "        ]\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving documents: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "YYuCw2Qr9Nsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity checks\n",
        "results = retrieve(query=\"What are the cafeteria plan benefits?.\", model_name=\"deepseek\", k=4,re_rank=True)"
      ],
      "metadata": {
        "id": "2ia0PX9H0_JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "H8Zvfz5T-upg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 4. Prepare 50 Test Queries\n",
        "# -------------------------------\n",
        "test_queries = [\n",
        "    \"How can I connect to Outlook Web?\", # 3985\n",
        "    \"How can I access my Officient Calendar?\", # 3986\n",
        "    \"Bannière?\", # 3987\n",
        "    \"What are the cafeteria plan benefits?\", # 3989\n",
        "    \"What about my car configuration offer?\", # 3990\n",
        "    \"How do I create a Canva?\", # 3991\n",
        "    \"What about chargemap business?\", # 3992\n",
        "    \"What is the Moodboard?\", # 3993\n",
        "    \"What about Chargemap (domicile)?\", # 3994\n",
        "    \"What about Connecting Expertise?\", # 3995\n",
        "    \"What's the BeCentral address?\", # 3996\n",
        "    \"What about a Microsoft 365 license?\", # 3997\n",
        "    \"What about Google Calendar\", # 3998\n",
        "    \"How to modify a page on dtsc.be?\", # 3999\n",
        "    \"What are compensatory rest days?\", # 4000\n",
        "    \"How do I access the shared library?\", # 4001\n",
        "    \"What is the login for StaffIT?\", # 4003\n",
        "    \"How can I export contacts from Odoo?\", # 4005\n",
        "    \"How can I export leads from Odoo?\", # 4006\n",
        "    \"What is the structure for OneDrive?\", # 4007\n",
        "    \"Who is responsible in case of a traffic fine?\", # 4008\n",
        "    \"What about dtsc.be performance?\", # 4010\n",
        "    \"What about mailing lists?\", # 4011\n",
        "    \"What about a green card?\", # 4014\n",
        "    \"Where is the Internship Agreement?\", # 4015\n",
        "    \"What about the company credit card?\", # 4016\n",
        "    \"How to create a teams meeting from Google Agenda?\", # 4017\n",
        "    \"What about Supplementary Family Allowances?\", # 4018\n",
        "    \"On what days does the company post on LinkedIn?\", # 4019\n",
        "    \"What activities are included in the DTeam Spirit Challenge?\", # 4020\n",
        "    \"What are the limits for the mobility budget?\", # 4021\n",
        "    \"What about Nexxtmove?\", # 4024\n",
        "    \"How to use Odoo for CRM?\", # 4025\n",
        "    \"What about Officient employee self-service?\" # 4026\n",
        "    \"What about the Onboarding To Do List?\", # 4027\n",
        "    \"What about birth leave?\", # 4028\n",
        "    \"What about dtsc.odoo.com?\", # 4030\n",
        "    \"What about ProUnity?\", # 4031\n",
        "    \"What about a hiring bonus?\", # 4032\n",
        "    \"What about Powerdale?\", # 4034\n",
        "    \"What about Single Permits?\", # 4035\n",
        "    \"What about the BNP application?\", # 4037\n",
        "    \"What about Elia?\", # 4038\n",
        "    \"What about Subsidies?\", # 4040\n",
        "    \"Who are our suppliers?\", # 4041\n",
        "    \"What is TED?\", # 4042\n",
        "    \"How to activate Music Streaming?\", # 4043\n",
        "    \"What is Scrum for?\", # 4046\n",
        "    \"How to add a Shared Mailbox?\", # 4047\n",
        "    \"What about BNP Paribas warrants?\" # 4048\n",
        "]"
      ],
      "metadata": {
        "id": "bDkFHQVZAWU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure exactly 50 queries.\n",
        "if len(test_queries) < 2:\n",
        "    test_queries = test_queries *( 2// len(test_queries) + 1)\n",
        "test_queries = test_queries[:2]\n",
        "print(\"Prepared {} test queries.\".format(len(test_queries)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToEB8VwPAlv9",
        "outputId": "f1fb693e-ece7-4d8f-8b5d-e9f567baa7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 2 test queries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 5. Retrieve Documents for Each Query Across All Models\n",
        "# -------------------------------\n",
        "# Initialize a list to collect data\n",
        "\n",
        "models = [\"gpt-4o-mini\",  \"claude\", \"mistral\", \"cohere\", \"deepseek\"]\n",
        "# retrieval_results structure: { model_name: { query: [list of document results] } }\n",
        "retrieval_results = {model: {} for model in models}\n",
        "\n",
        "for model in models:\n",
        "    for query in test_queries:\n",
        "        # Set re_rank=True if you wish to re-rank documents using the LLM.\n",
        "        retrieval_results[model][query] = retrieve(query, model, k=4, re_rank=True)\n",
        "print(\"Retrieval complete for all models and queries.\")\n"
      ],
      "metadata": {
        "id": "lLV2ugNxArEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_language(content):\n",
        "    \"\"\"\n",
        "    Detects the language of a given text content.\n",
        "\n",
        "    Args:\n",
        "        content (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: Detected language code (e.g., 'en', 'fr', 'de', 'nl'), or None if detection fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return detect(content)\n",
        "    except Exception:\n",
        "        return None"
      ],
      "metadata": {
        "id": "HjFDTfjxArAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to collect data\n",
        "data = []\n",
        "\n",
        "\n",
        "for model, queries in retrieval_results.items():\n",
        "    for query, documents in queries.items():\n",
        "        for doc in documents:\n",
        "            content = doc.get('content', '')\n",
        "            if isinstance(content, str) and content.strip():\n",
        "                language = detect_language(content)\n",
        "            else:\n",
        "                language = None\n",
        "            data.append((model, query, doc['rank'], doc['id'], language))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df = pd.DataFrame(data, columns=['Model', 'Query', 'Rank', 'Document ID', 'Language'])"
      ],
      "metadata": {
        "id": "5XiiZ9DNAq8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the plot grid: 2x2 for ranks 1 to 4\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "#fig.suptitle(\"Language Category Distribution by Model for Each Rank\", fontsize=16)\n",
        "\n",
        "# Flatten axes array for easy indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through Rank 1 to 4\n",
        "for rank in range(1, 5):\n",
        "    ax = axes[rank - 1]\n",
        "    subset = df[df[\"Rank\"] == rank]\n",
        "    sns.countplot(data=subset, x=\"Language\", hue=\"Model\", order=[\"en\", \"fr\", \"nl\", \"de\"], ax=ax, palette= model_palette)\n",
        "    ax.set_title(f\"Rank {rank}\",fontsize = 12)\n",
        "    ax.set_xlabel(\"Language\", fontsize = 15)\n",
        "    ax.set_ylabel(\"Count\", fontsize = 15)\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "fig.savefig(\"retrieval_language_distribution.png\", dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "Bl1awXn9Aq52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the plot grid: 2x2 for ranks 1 to 4\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "#fig.suptitle(\"Language Category Distribution by Model for Each Rank\", fontsize=16)\n",
        "\n",
        "# Flatten axes array for easy indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through Rank 1 to 4\n",
        "for rank in range(1, 5):\n",
        "    ax = axes[rank - 1]\n",
        "    subset = df[df[\"Rank\"] == rank]\n",
        "    sns.countplot(data=subset, x=\"Language\", hue=\"Model\", order=[\"en\", \"fr\", \"nl\", \"de\"], ax=ax, palette= model_palette)\n",
        "    ax.set_title(f\"Rank {rank}\",fontsize = 12)\n",
        "    ax.set_xlabel(\"Language\", fontsize = 15)\n",
        "    ax.set_ylabel(\"Count\", fontsize = 15)\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "fig.savefig(\"retrieval_language_distribution.png\", dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "kug-cBdRGzxv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}