{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tutorial Retrieval Recency Bias\n",
    "\n",
    "In this tutorial, we will explore how to test the recency of retrieval results using different LLMs. We will use a sample dataset and a set of test queries to evaluate the performance of various models in retrieving relevant documents based on their recency."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Import necessary libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:39:36.808109Z",
     "start_time": "2025-05-16T14:39:36.801642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from unbiasai.config import DATA_DIR\n",
    "from unbiasai.utils import initialize_llm, generate_embeddings, insert_documents, retrieve, extract_created_datetime\n",
    "from supabase import create_client\n",
    "from queries.retrieval_recency import test_queries\n",
    "load_dotenv()\n",
    "\n",
    "# Load data from CSV\n",
    "file_path = DATA_DIR / 'retrieval_recency.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.head(10)  # For testing, limit to 10 rows\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Initialize models and store them in a dictionary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:39:36.843436Z",
     "start_time": "2025-05-16T14:39:36.818113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\"gpt\", \"claude\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "models = [\"gpt\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "\n",
    "initialized_models = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    initialized_models[model_name] = initialize_llm(model_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt\n",
      "Initializing model: gpt\n",
      "    LLM initialized correctly\n",
      "mistral\n",
      "Initializing model: mistral\n",
      "    LLM initialized correctly\n",
      "cohere\n",
      "Initializing model: cohere\n",
      "    LLM initialized correctly\n",
      "deepseek\n",
      "Initializing model: deepseek\n",
      "    LLM initialized correctly\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Connect to Supabase"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:39:36.858758Z",
     "start_time": "2025-05-16T14:39:36.853609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Supabase client\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Generate Embeddings for the Data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your new df including the embeddings in the supabase table to create a vector store."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:39:44.613604Z",
     "start_time": "2025-05-16T14:39:36.866099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['embedding'] = df['content'].apply(generate_embeddings)\n",
    "\n",
    "# IMPORTANT: change function so supabase table name can be changed.\n",
    "#\n",
    "insert_documents(df, supabase_client)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting document with ID: 3984\n",
      "Inserting document with ID: 3984\n",
      "Inserting document with ID: 3984\n",
      "Inserting document with ID: 3984\n",
      "Inserting document with ID: 3985\n",
      "Inserting document with ID: 3986\n",
      "Inserting document with ID: 3986\n",
      "Inserting document with ID: 3986\n",
      "Inserting document with ID: 3986\n",
      "Inserting document with ID: 3987\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5. Define your Test Queries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:39:44.637765Z",
     "start_time": "2025-05-16T14:39:44.636083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import queries\n",
    "from queries.retrieval_recency import test_queries\n",
    "\n",
    "# or define your own queries\n",
    "test_queries = [\n",
    "    \"What is test query 1?\",\n",
    "    \"What is test query 2?\",\n",
    "    \"What is test query 3?\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6. Retrieve and Rerank Documents for Each Query Across All Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:40:44.543760Z",
     "start_time": "2025-05-16T14:39:44.650946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_results = {}\n",
    "for model_name, model in initialized_models.items():\n",
    "    print(f\"Running retrieval with model: {model_name}\")\n",
    "    retrieval_results[model_name] = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"  Processing query: {query[:30]}...\")\n",
    "        retrieval_results[model_name][query] = retrieve(\n",
    "            query, model, supabase_client, k=4, re_rank=True\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Completed all queries for {model_name}\")\n",
    "\n",
    "print(\"Retrieval complete for all models and queries.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running retrieval with model: gpt\n",
      "  Processing query: What is test query 1?...\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: 2,1,3,0. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "✓ Completed all queries for gpt\n",
      "Running retrieval with model: mistral\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: The query is \"What is test query 1?\" and none of the documents seem to directly address this query. However, based on the content provided, we can infer that the documents discussing \"Quarterly Performance Review Process\" are more relevant to a general understanding of processes within the company, which might be related to \"test query 1\" in some indirect way. Document 1 seems the least relevant as it discusses accounting topics.\n",
      "\n",
      "Given this, the re-ranked order from most to least relevant would be:\n",
      "\n",
      "**3,2,4,1**. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: Based on the query \"What is test query 2?\" and the provided documents, it seems there might be some confusion or a placeholder in the query. However, I will rank the documents based on their relevance to a general understanding of what a \"test query\" might entail, assuming it refers to a process or procedure within the company's systems.\n",
      "\n",
      "Here's the ranking:\n",
      "\n",
      "1. **Document 2 (ID: 0)**: This document discusses accessing shared developer environments, which could be relevant if \"test query 2\" refers to a specific procedure or access method within the company's systems.\n",
      "2. **Document 3 (ID: 3)**: Similar to Document 2, this document also discusses accessing shared developer environments, making it relevant for the same reasons.\n",
      "3. **Document 4 (ID: 31)**: This document talks about the quarterly performance review process, which might be relevant if \"test query 2\" is related to performance metrics or reviews.\n",
      "4. **Document 1 (ID: 3985)**: This document seems to be about accounting processes (purchases, sales, invoices, payments, etc.), which is the least relevant to the concept of a \"test query.\"\n",
      "\n",
      "So, the re-ranked order is: **2,3,4,1**.. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: The query is \"What is test query 3?\" None of the documents provided seem to directly address this query. However, based on the content, we can infer that the documents discussing \"Quarterly Performance Review Process\" are more likely to be related to some form of structured process or review, which might be loosely connected to the idea of a \"test query\" in a general sense. Document 1, which discusses accounting topics, seems the least relevant.\n",
      "\n",
      "Given this, the re-ranked order from most to least relevant would be:\n",
      "\n",
      "2,3,4,1. Using default order.\n",
      "✓ Completed all queries for mistral\n",
      "Running retrieval with model: cohere\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: Since the query is \"What is test query 1?\" and none of the documents contain information related to \"test query 1\", all documents are equally irrelevant. However, I will rank them based on the assumption that the query might be related to the most recent or detailed information.\n",
      "\n",
      "Given the documents are about quarterly performance reviews and one is from 2025, which is the most recent, I will prioritize that one. The other three documents are identical in content but differ in dates, so I will rank them by recency.\n",
      "\n",
      "**Re-ranked order:** 4,2,3,1\n",
      "\n",
      "Note: This ranking is based on an assumption, as the query is not related to the content of the documents. If the query were related to the content, the ranking would be different.. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: Since the query \"What is test query 2?\" is not related to any of the provided documents, and none of the documents contain information about \"test query 2,\" all documents are equally irrelevant. However, I must provide a ranking as per the instructions. Given the lack of relevance, I will rank them based on their document IDs in ascending order, which is an arbitrary but necessary choice in this context.\n",
      "\n",
      "**3,1,2,3** \n",
      "\n",
      "Note: This ranking is arbitrary due to the lack of relevance of all documents to the query.. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: Since the query is \"What is test query 3?\" and none of the documents contain any information related to \"test query 3\", all documents are equally irrelevant. However, I will provide the ranking based on the given instructions:\n",
      "\n",
      "3,2,4,1. Using default order.\n",
      "✓ Completed all queries for cohere\n",
      "Running retrieval with model: deepseek\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: The query \"What is test query 1?\" is very general and does not provide specific context or keywords to match against the documents. However, based on the content of the documents:\n",
      "\n",
      "1. Document 1 (ID: 3985) contains terms like \"Quaterly closing\" and \"Annual closing,\" which might loosely relate to a \"test query\" about periodic processes.\n",
      "2. Documents 2, 3, and 4 (IDs: 31, 33, 30) all discuss \"Quarterly Performance Review Process,\" which is less relevant to the query.\n",
      "\n",
      "Given this, the most relevant document is Document 1, while the others are equally less relevant. Since Documents 2, 3, and 4 are nearly identical in content, their order can be based on recency (last modified date):\n",
      "\n",
      "4,2,3,1\n",
      "\n",
      "However, since the query is very vague, this ranking is speculative. A more precise query would yield better results. \n",
      "\n",
      "Final ranking: 1,4,2,3. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: The query is \"What is test query 2?\" and none of the documents directly address this query. However, based on the content of the documents, here is a ranking from most to least relevant:\n",
      "\n",
      "1. Document 2 (ID: 31) - This document discusses a structured process (quarterly reviews), which might loosely relate to a \"test query\" in terms of evaluation or assessment.\n",
      "2. Document 3 (ID: 0) - This document is about accessing shared developer environments, which is less relevant but still involves some form of query or access.\n",
      "3. Document 4 (ID: 3) - Similar to Document 3 but older, making it slightly less relevant.\n",
      "4. Document 1 (ID: 3985) - This document is about accounting topics and is the least relevant to the query.\n",
      "\n",
      "Final ranking: 31,0,3,3985. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: The query is \"What is test query 3?\" and none of the documents provided directly address this query. However, based on the content of the documents:\n",
      "\n",
      "1. Document 1 (ID: 3985) mentions \"Quaterly closing\" and \"Annual closing,\" which might be tangentially related to a test query about processes or reviews.\n",
      "2. Documents 2, 3, and 4 (IDs: 33, 32, 31) are all identical in content, discussing the \"Quarterly Performance Review Process,\" which is not relevant to the query.\n",
      "\n",
      "Since none of the documents directly answer the query, the ranking is based on the possibility of tangential relevance. \n",
      "\n",
      "Final ranking: 3985,33,32,31. Using default order.\n",
      "✓ Completed all queries for deepseek\n",
      "Retrieval complete for all models and queries.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Process the Rankings\n",
    "Define 'pattern' to match the 'created' date with regex."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:40:44.571926Z",
     "start_time": "2025-05-16T14:40:44.567642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize a list to collect data\n",
    "data = []\n",
    "\n",
    "# Iterate over each model and its corresponding queries\n",
    "for model, queries in retrieval_results.items():\n",
    "    for query, documents in queries.items():\n",
    "        for doc in documents:\n",
    "            created_datetime = extract_created_datetime(doc['content'], pattern=r'createdDateTime[\":]*(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z)')\n",
    "            data.append((model, query, doc['rank'], doc['id'], created_datetime))\n",
    "\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data, columns=['Model', 'Query', 'Rank', 'Document ID', 'Created DateTime'])"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the date categories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:40:44.585080Z",
     "start_time": "2025-05-16T14:40:44.581183Z"
    }
   },
   "source": [
    "# Define the date categories\n",
    "date_categories = ['newest', 'newer', 'older', 'oldest']\n",
    "\n",
    "# Sort and assign date categories within each group\n",
    "df['date_category'] = (\n",
    "    df.sort_values(by='Created DateTime', ascending=False)\n",
    "    .groupby(['Model', 'Query'])\n",
    "    .cumcount()\n",
    "    .map({i: category for i, category in enumerate(date_categories)})\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:40:44.598756Z",
     "start_time": "2025-05-16T14:40:44.594896Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Model                  Query  Rank  Document ID    Created DateTime  \\\n",
       "0   gpt  What is test query 1?     1           33 2020-09-14 08:50:17   \n",
       "1   gpt  What is test query 1?     2           31 2024-06-18 15:35:22   \n",
       "2   gpt  What is test query 1?     3           30 2025-02-27 09:10:45   \n",
       "3   gpt  What is test query 1?     4         3985 2022-11-09 16:04:18   \n",
       "4   gpt  What is test query 2?     1         3985 2022-11-09 16:04:18   \n",
       "\n",
       "  date_category  \n",
       "0        oldest  \n",
       "1         newer  \n",
       "2        newest  \n",
       "3         older  \n",
       "4         older  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Query</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Created DateTime</th>\n",
       "      <th>date_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2020-09-14 08:50:17</td>\n",
       "      <td>oldest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-06-18 15:35:22</td>\n",
       "      <td>newer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>2025-02-27 09:10:45</td>\n",
       "      <td>newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>4</td>\n",
       "      <td>3985</td>\n",
       "      <td>2022-11-09 16:04:18</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 2?</td>\n",
       "      <td>1</td>\n",
       "      <td>3985</td>\n",
       "      <td>2022-11-09 16:04:18</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:40:44.628099Z",
     "start_time": "2025-05-16T14:40:44.626734Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
