{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tutorial Retrieval Recency Bias\n",
    "\n",
    "In this tutorial, we will explore how to test the recency of retrieval results using different LLMs. We will use a sample dataset and a set of test queries to evaluate the performance of various models in retrieving relevant documents based on their recency."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Import necessary libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:57:53.824918Z",
     "start_time": "2025-05-16T14:57:52.803418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from unbiasai.config import DATA_DIR\n",
    "from unbiasai.utils import initialize_llm, generate_embeddings, insert_documents, retrieve, extract_created_datetime\n",
    "from supabase import create_client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load data from CSV\n",
    "file_path = DATA_DIR / 'retrieval_recency.csv'\n",
    "df = pd.read_csv(file_path)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Initialize models and store them in a dictionary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:57:53.954813Z",
     "start_time": "2025-05-16T14:57:53.907818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\"gpt\", \"claude\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "models = [\"gpt\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "\n",
    "initialized_models = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    initialized_models[model_name] = initialize_llm(model_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt\n",
      "Initializing model: gpt\n",
      "    LLM initialized correctly\n",
      "mistral\n",
      "Initializing model: mistral\n",
      "    LLM initialized correctly\n",
      "cohere\n",
      "Initializing model: cohere\n",
      "    LLM initialized correctly\n",
      "deepseek\n",
      "Initializing model: deepseek\n",
      "    LLM initialized correctly\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Connect to Supabase"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:57:53.967390Z",
     "start_time": "2025-05-16T14:57:53.961673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Supabase client\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Generate Embeddings for the Data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your new df including the embeddings in the supabase table to create a vector store."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:57:58.531975Z",
     "start_time": "2025-05-16T14:57:53.973966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['embedding'] = df['content'].apply(generate_embeddings)\n",
    "\n",
    "# IMPORTANT: change function so supabase table name can be changed.\n",
    "#\n",
    "insert_documents(df, supabase_client)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting document with ID: 0\n",
      "Inserting document with ID: 1\n",
      "Inserting document with ID: 2\n",
      "Inserting document with ID: 3\n",
      "Inserting document with ID: 4\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5. Define your Test Queries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:57:58.564521Z",
     "start_time": "2025-05-16T14:57:58.561714Z"
    }
   },
   "cell_type": "code",
   "source": "from queries.retrieval_recency import test_queries",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6. Retrieve and Rerank Documents for Each Query Across All Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:58:47.247507Z",
     "start_time": "2025-05-16T14:57:58.575673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_results = {}\n",
    "for model_name, model in initialized_models.items():\n",
    "    print(f\"Running retrieval with model: {model_name}\")\n",
    "    retrieval_results[model_name] = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"  Processing query: {query[:30]}...\")\n",
    "        retrieval_results[model_name][query] = retrieve(\n",
    "            query, model, supabase_client, k=4, re_rank=True\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Completed all queries for {model_name}\")\n",
    "\n",
    "print(\"Retrieval complete for all models and queries.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running retrieval with model: gpt\n",
      "  Processing query: What is test query 1?...\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: 2,1,0,3. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: 2,1,0,4. Using default order.\n",
      "✓ Completed all queries for gpt\n",
      "Running retrieval with model: mistral\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: The query is \"What is test query 1?\" and none of the documents provided seem to directly address this query. However, we can rank the documents based on the relevance of their metadata and content to the query. Since the query is very generic and doesn't match any specific content in the documents, we can consider the recency of the documents as a factor for relevance.\n",
      "\n",
      "Here is the ranking from most to least relevant based on the recency of the documents:\n",
      "\n",
      "1. Document 1 (ID: 2) - Created and last modified in 2023\n",
      "2. Document 3 (ID: 1) - Created and last modified in 2024\n",
      "3. Document 2 (ID: 0) - Created and last modified in 2025\n",
      "4. Document 4 (ID: 3985) - Created in 2022, last modified in 2024\n",
      "\n",
      "So, the re-ranked order is: **2,1,0,3985**. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: Given the query \"What is test query 2?\" and the provided documents, none of the documents seem to directly address the query. However, we can rank them based on the recency of the `createdDateTime` field, assuming that more recent documents might be more relevant in the absence of other contextual information.\n",
      "\n",
      "Here is the ranking from most to least relevant based on the `createdDateTime`:\n",
      "\n",
      "1. Document 1 (ID: 0): 2025-02-19\n",
      "2. Document 3 (ID: 1): 2024-02-19\n",
      "3. Document 2 (ID: 2): 2023-02-19\n",
      "4. Document 4 (ID: 3): 2022-02-19\n",
      "\n",
      "So the re-ranked order is: **0,1,2,3**. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: The query is \"What is test query 3?\". Given the documents provided, none of them directly answer the query as they all contain metadata about a specific output ID created by John Taylor. However, if we are to re-rank them based on the recency of the `createdDateTime` field, assuming that more recent documents might be more relevant in a general sense, the ranking would be as follows:\n",
      "\n",
      "1. Document 2 (ID: 0) - Created in 2025\n",
      "2. Document 3 (ID: 1) - Created in 2024\n",
      "3. Document 1 (ID: 2) - Created in 2023\n",
      "4. Document 4 (ID: 4) - Created in 2021\n",
      "\n",
      "So the re-ranked order from most to least relevant based on recency is:\n",
      "\n",
      "**0,1,2,4**. Using default order.\n",
      "✓ Completed all queries for mistral\n",
      "Running retrieval with model: cohere\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: Since the query \"What is test query 1?\" is not related to any specific information in the documents, and the documents themselves do not contain any content that directly answers the query, the relevance of the documents is based on the assumption that the query might be looking for the most recent or relevant information in general.\n",
      "\n",
      "However, given the lack of direct relevance, I will rank the documents based on the assumption that the query might be looking for the most recent information related to the output.id \"39b7d18e-7b92-461b-b0e6-cac12b67f769\". \n",
      "\n",
      "Considering the createdDateTime and lastModifiedDateTime, the most recent document is Document 2 (ID: 0), followed by Document 3 (ID: 1), then Document 1 (ID: 2). Document 4 (ID: 3985) has a different output.id and is not related to the query.\n",
      "\n",
      "**Re-ranked order:** 0,1,2,3985. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: Since the query \"What is test query 2?\" is not related to the content of the documents, and all documents are identical except for the dates, I will rank them based on the assumption that the most recent document might be the most relevant. \n",
      "\n",
      "3,1,2,0. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: Since the query \"What is test query 3?\" does not provide any specific information to compare with the documents, and all documents are identical except for the dates, I will rank them based on the assumption that the most recent information is the most relevant.\n",
      "\n",
      "However, since the query is unclear, I will provide a ranking based on the most recent date first.\n",
      "\n",
      "2,1,0,4. Using default order.\n",
      "✓ Completed all queries for cohere\n",
      "Running retrieval with model: deepseek\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: Given the query \"What is test query 1?\", none of the provided documents appear to directly address the query. However, based on the content and metadata, here is a re-ranking from most to least relevant:\n",
      "\n",
      "1. Document 1 (ID: 2) - Most recent among the first three similar documents.\n",
      "2. Document 3 (ID: 1) - Next most recent among the first three similar documents.\n",
      "3. Document 2 (ID: 0) - Oldest among the first three similar documents.\n",
      "4. Document 4 (ID: 3985) - Unrelated content about accounting, least relevant to the query.\n",
      "\n",
      "Final ranking: 2,1,0,3985. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: Given that all documents are identical in content except for their timestamps, and none of them contain any information related to the query \"What is test query 2?\", the relevance ranking is arbitrary. However, a common approach is to rank them by the most recent to the least recent based on the `createdDateTime`. \n",
      "\n",
      "Here is the ranking from most to least relevant based on the `createdDateTime`:\n",
      "\n",
      "1,3,2,4. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: Given that all documents are identical in content except for their timestamps, and none of them contain any information related to the query \"What is test query 3?\", none of the documents are relevant to the query. However, if we were to rank them based on the most recent timestamps (assuming more recent documents might be marginally more relevant by default), the order would be:\n",
      "\n",
      "2,0,1,4. Using default order.\n",
      "✓ Completed all queries for deepseek\n",
      "Retrieval complete for all models and queries.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Process the Rankings\n",
    "Define 'pattern' to match the 'created' date with regex."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:58:47.266771Z",
     "start_time": "2025-05-16T14:58:47.262748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize a list to collect data\n",
    "data = []\n",
    "\n",
    "# Iterate over each model and its corresponding queries\n",
    "for model, queries in retrieval_results.items():\n",
    "    for query, documents in queries.items():\n",
    "        for doc in documents:\n",
    "            created_datetime = extract_created_datetime(doc['content'], pattern=r'createdDateTime[\":]*(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z)')\n",
    "            data.append((model, query, doc['rank'], doc['id'], created_datetime))\n",
    "\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data, columns=['Model', 'Query', 'Rank', 'Document ID', 'Created DateTime'])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the date categories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:58:47.278549Z",
     "start_time": "2025-05-16T14:58:47.274834Z"
    }
   },
   "source": [
    "# Define the date categories\n",
    "date_categories = ['newest', 'newer', 'older', 'oldest']\n",
    "\n",
    "# Sort and assign date categories within each group\n",
    "df['date_category'] = (\n",
    "    df.sort_values(by='Created DateTime', ascending=False)\n",
    "    .groupby(['Model', 'Query'])\n",
    "    .cumcount()\n",
    "    .map({i: category for i, category in enumerate(date_categories)})\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:58:47.295808Z",
     "start_time": "2025-05-16T14:58:47.288047Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Model                  Query  Rank  Document ID    Created DateTime  \\\n",
       "0   gpt  What is test query 1?     1            2 2023-02-19 15:20:19   \n",
       "1   gpt  What is test query 1?     2            0 2025-02-19 15:20:19   \n",
       "2   gpt  What is test query 1?     3            1 2024-02-19 15:20:19   \n",
       "3   gpt  What is test query 1?     4         3985 2022-11-09 16:04:18   \n",
       "4   gpt  What is test query 2?     1            0 2025-02-19 15:20:19   \n",
       "\n",
       "  date_category  \n",
       "0         older  \n",
       "1        newest  \n",
       "2         newer  \n",
       "3        oldest  \n",
       "4        newest  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Query</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Created DateTime</th>\n",
       "      <th>date_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-19 15:20:19</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-19 15:20:19</td>\n",
       "      <td>newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-02-19 15:20:19</td>\n",
       "      <td>newer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>4</td>\n",
       "      <td>3985</td>\n",
       "      <td>2022-11-09 16:04:18</td>\n",
       "      <td>oldest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 2?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-19 15:20:19</td>\n",
       "      <td>newest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:58:47.319758Z",
     "start_time": "2025-05-16T14:58:47.318093Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
