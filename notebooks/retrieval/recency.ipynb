{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tutorial Retrieval Recency Bias\n",
    "\n",
    "In this tutorial, we will explore how to test the recency of retrieval results using different LLMs. We will use a sample dataset and a set of test queries to evaluate the performance of various models in retrieving relevant documents based on their recency."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Import necessary libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:51.302316Z",
     "start_time": "2025-05-16T14:54:50.283688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from unbiasai.config import DATA_DIR\n",
    "from unbiasai.utils import initialize_llm, generate_embeddings, insert_documents, retrieve, extract_created_datetime\n",
    "from supabase import create_client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load data from CSV\n",
    "file_path = DATA_DIR / 'retrieval_recency.csv'\n",
    "df = pd.read_csv(file_path)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Initialize models and store them in a dictionary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:51.432Z",
     "start_time": "2025-05-16T14:54:51.383843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\"gpt\", \"claude\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "models = [\"gpt\", \"mistral\", \"cohere\", \"deepseek\"]\n",
    "\n",
    "initialized_models = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    initialized_models[model_name] = initialize_llm(model_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt\n",
      "Initializing model: gpt\n",
      "    LLM initialized correctly\n",
      "mistral\n",
      "Initializing model: mistral\n",
      "    LLM initialized correctly\n",
      "cohere\n",
      "Initializing model: cohere\n",
      "    LLM initialized correctly\n",
      "deepseek\n",
      "Initializing model: deepseek\n",
      "    LLM initialized correctly\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Connect to Supabase"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:51.444020Z",
     "start_time": "2025-05-16T14:54:51.438639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Supabase client\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Generate Embeddings for the Data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your new df including the embeddings in the supabase table to create a vector store."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.506035Z",
     "start_time": "2025-05-16T14:54:51.452802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['embedding'] = df['content'].apply(generate_embeddings)\n",
    "\n",
    "# IMPORTANT: change function so supabase table name can be changed.\n",
    "#\n",
    "insert_documents(df, supabase_client)\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'text': 'float' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33membedding\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcontent\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerate_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# IMPORTANT: change function so supabase table name can be changed.\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m      5\u001B[39m insert_documents(df, supabase_client)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/pandas/core/series.py:4924\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4789\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4790\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4791\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4796\u001B[39m     **kwargs,\n\u001B[32m   4797\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4798\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4799\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4800\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4915\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4916\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4917\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4919\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4920\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4921\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4922\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4924\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1424\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1426\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1427\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1501\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1502\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1503\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1504\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1505\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1506\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1507\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1508\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1509\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1512\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1513\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1514\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/pandas/core/base.py:921\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    918\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    919\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2972\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/src/unbiasai/utils.py:47\u001B[39m, in \u001B[36mgenerate_embeddings\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     43\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY environment variable not set\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     44\u001B[39m embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,\n\u001B[32m     45\u001B[39m                                model=\u001B[33m\"\u001B[39m\u001B[33mtext-embedding-3-large\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43membeddings\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:617\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_query\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    608\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[32m    609\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001B[39;00m\n\u001B[32m    610\u001B[39m \n\u001B[32m    611\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    615\u001B[39m \u001B[33;03m        Embedding for the text.\u001B[39;00m\n\u001B[32m    616\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:576\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    574\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    575\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:468\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    452\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    453\u001B[39m \u001B[33;03mGenerate length-safe embeddings for a list of texts.\u001B[39;00m\n\u001B[32m    454\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    465\u001B[39m \u001B[33;03m    List[List[float]]: A list of embeddings for each input text.\u001B[39;00m\n\u001B[32m    466\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    467\u001B[39m _chunk_size = chunk_size \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunk_size\n\u001B[32m--> \u001B[39m\u001B[32m468\u001B[39m _iter, tokens, indices = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    469\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    470\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:429\u001B[39m, in \u001B[36mOpenAIEmbeddings._tokenize\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    427\u001B[39m     token = encoding.encode(text, **encoder_kwargs)\n\u001B[32m    428\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m429\u001B[39m     token = \u001B[43mencoding\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_ordinary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    431\u001B[39m \u001B[38;5;66;03m# Split tokens into chunks respecting the embedding_ctx_length\u001B[39;00m\n\u001B[32m    432\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(token), \u001B[38;5;28mself\u001B[39m.embedding_ctx_length):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work-MBP/UnBiasAI/.venv/lib/python3.13/site-packages/tiktoken/core.py:73\u001B[39m, in \u001B[36mEncoding.encode_ordinary\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Encodes a string into tokens, ignoring special tokens.\u001B[39;00m\n\u001B[32m     65\u001B[39m \n\u001B[32m     66\u001B[39m \u001B[33;03mThis is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     70\u001B[39m \u001B[33;03m[31373, 995]\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_core_bpe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_ordinary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mUnicodeEncodeError\u001B[39;00m:\n\u001B[32m     75\u001B[39m     \u001B[38;5;66;03m# See comment in encode\u001B[39;00m\n\u001B[32m     76\u001B[39m     text = text.encode(\u001B[33m\"\u001B[39m\u001B[33mutf-16\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33msurrogatepass\u001B[39m\u001B[33m\"\u001B[39m).decode(\u001B[33m\"\u001B[39m\u001B[33mutf-16\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mreplace\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mTypeError\u001B[39m: argument 'text': 'float' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5. Define your Test Queries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.507868Z",
     "start_time": "2025-05-16T14:47:22.889916Z"
    }
   },
   "cell_type": "code",
   "source": "from queries.retrieval_recency import test_queries",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6. Retrieve and Rerank Documents for Each Query Across All Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.508943Z",
     "start_time": "2025-05-16T14:47:22.902087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_results = {}\n",
    "for model_name, model in initialized_models.items():\n",
    "    print(f\"Running retrieval with model: {model_name}\")\n",
    "    retrieval_results[model_name] = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"  Processing query: {query[:30]}...\")\n",
    "        retrieval_results[model_name][query] = retrieve(\n",
    "            query, model, supabase_client, k=4, re_rank=True\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Completed all queries for {model_name}\")\n",
    "\n",
    "print(\"Retrieval complete for all models and queries.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running retrieval with model: gpt\n",
      "  Processing query: What is test query 1?...\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: 2,1,3,0. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "✓ Completed all queries for gpt\n",
      "Running retrieval with model: mistral\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: The query is \"What is test query 1?\" None of the documents provided seem to be directly relevant to this query, as they all discuss topics related to accounting processes and quarterly performance reviews. However, if we were to rank them based on the mention of \"quarterly\" which is a keyword in the query, the ranking would be as follows:\n",
      "\n",
      "1. Document 2 (ID: 31) - Mentions \"Quarterly Performance Review Process\" and has the most recent creation date.\n",
      "2. Document 3 (ID: 33) - Also mentions \"Quarterly Performance Review Process\" but has an older creation date.\n",
      "3. Document 4 (ID: 30) - Mentions \"Quarterly Performance Review Process\" but has a future creation date.\n",
      "4. Document 1 (ID: 3985) - Mentions \"Quaterly closing\" (note the misspelling) but is not directly related to the query.\n",
      "\n",
      "So, the re-ranked order is: **2,3,4,1**.. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: The query \"What is test query 2?\" does not provide any meaningful context or keywords to determine the relevance of the documents. However, based on the content provided, I will rank the documents based on the likelihood of them being related to a generic \"test query\" scenario, assuming it might be related to processes or instructions.\n",
      "\n",
      "Here is the ranking from most to least relevant:\n",
      "\n",
      "1. **Document 2 (ID: 31)**: This document discusses a structured process (quarterly performance reviews), which might be related to a \"test query\" in the context of a process or procedure.\n",
      "\n",
      "2. **Document 3 (ID: 0)**: This document provides step-by-step instructions for accessing shared developer environments, which could be relevant if \"test query\" is related to technical processes or environments.\n",
      "\n",
      "3. **Document 4 (ID: 3)**: This document is very similar to Document 3, providing instructions for accessing shared developer environments. Since it is a duplicate of Document 3, it is less relevant.\n",
      "\n",
      "4. **Document 1 (ID: 3985)**: This document lists financial terms and processes (purchases, sales, invoices, payments, quarterly closing, annual closing), which are less likely to be directly related to a generic \"test query.\"\n",
      "\n",
      "So, the re-ranked order is: **2,3,4,1**.. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: The query is \"What is test query 3?\" and there is no context provided for what \"test query 3\" refers to. However, based on the documents provided, we can infer that the most relevant documents would be those that discuss processes or tasks that occur quarterly, as \"query 3\" might be inferred to refer to a quarterly process or task.\n",
      "\n",
      "Given this, the documents discussing \"Quarterly Performance Review Process\" are more relevant than the document discussing accounting tasks, which do not explicitly mention anything quarterly.\n",
      "\n",
      "Since Documents 2, 3, and 4 all discuss the same \"Quarterly Performance Review Process,\" they are equally relevant. Document 1 is the least relevant as it does not mention anything quarterly.\n",
      "\n",
      "The re-ranked order of documents from most to least relevant is:\n",
      "\n",
      "**3,2,4,1**. Using default order.\n",
      "✓ Completed all queries for mistral\n",
      "Running retrieval with model: cohere\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: Since the query is \"What is test query 1?\" and none of the documents contain information related to \"test query 1\", all documents are equally irrelevant. However, I will provide a ranking based on the assumption that the query is unrelated to the content of the documents.\n",
      "\n",
      "2,3,4,1. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: Since the query \"What is test query 2?\" is unclear and does not provide any specific information to match against the documents, none of the documents are relevant to the query. However, I must provide a ranking as per the instructions. Given the lack of relevance, I will rank them based on their document IDs in ascending order, which is an arbitrary choice in this case.\n",
      "\n",
      "0,3,31,3985. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: Since the query \"What is test query 3?\" does not provide any specific information or context, and none of the documents mention \"test query 3,\" it is impossible to determine relevance. However, based on the requirement to return exactly 4 document numbers, I will list them in the order they were provided, as there is no basis for re-ranking:\n",
      "\n",
      "2,3,4,1. Using default order.\n",
      "✓ Completed all queries for cohere\n",
      "Running retrieval with model: deepseek\n",
      "  Processing query: What is test query 1?...\n",
      "Invalid ranking received: The query \"What is test query 1?\" is very general and does not provide specific context. Given the documents provided:\n",
      "\n",
      "1. Document 1 is about accounting topics (purchases, sales, invoices, etc.), which is unrelated to the query.\n",
      "2. Documents 2, 3, and 4 are all identical in content (Quarterly Performance Review Process) and differ only in their metadata (dates). Since the content is identical, we can rank them by their last modified date (newest to oldest) as a tiebreaker.\n",
      "\n",
      "Thus, the ranking is: 4,2,3,1. Using default order.\n",
      "  Processing query: What is test query 2?...\n",
      "Invalid ranking received: The query is \"What is test query 2?\" and none of the documents directly address this query. However, based on the content of the documents, here is a ranking from most to least relevant:\n",
      "\n",
      "1. Document 2 (ID: 31) - Although not directly related to the query, it discusses a structured process (quarterly reviews), which might be loosely related to a \"test query\" in terms of evaluation.\n",
      "2. Document 3 (ID: 0) - Discusses accessing shared developer environments, which is not relevant but more technical.\n",
      "3. Document 4 (ID: 3) - Similar to Document 3 but older, making it less relevant.\n",
      "4. Document 1 (ID: 3985) - Focuses on accounting topics, which are least relevant to the query.\n",
      "\n",
      "Final ranking: 31,0,3,3985. Using default order.\n",
      "  Processing query: What is test query 3?...\n",
      "Invalid ranking received: The query is \"What is test query 3?\" and none of the documents provide any information related to this query. However, since the task requires ranking the documents, I will rank them based on their content and metadata, even though none are relevant to the query.\n",
      "\n",
      "1. Document 1 (ID: 3985) - This document is about accounting topics like purchases, sales, invoices, etc. It is the only document with content different from the others.\n",
      "2. Document 4 (ID: 31) - This is the most recently modified document among the remaining three, which are all identical in content.\n",
      "3. Document 3 (ID: 32) - This is the second most recently modified document among the identical ones.\n",
      "4. Document 2 (ID: 33) - This is the oldest document among the identical ones.\n",
      "\n",
      "Final ranking: 3985,31,32,33. Using default order.\n",
      "✓ Completed all queries for deepseek\n",
      "Retrieval complete for all models and queries.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Process the Rankings\n",
    "Define 'pattern' to match the 'created' date with regex."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.509088Z",
     "start_time": "2025-05-16T14:48:22.106551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize a list to collect data\n",
    "data = []\n",
    "\n",
    "# Iterate over each model and its corresponding queries\n",
    "for model, queries in retrieval_results.items():\n",
    "    for query, documents in queries.items():\n",
    "        for doc in documents:\n",
    "            created_datetime = extract_created_datetime(doc['content'], pattern=r'createdDateTime[\":]*(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z)')\n",
    "            data.append((model, query, doc['rank'], doc['id'], created_datetime))\n",
    "\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data, columns=['Model', 'Query', 'Rank', 'Document ID', 'Created DateTime'])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the date categories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.509915Z",
     "start_time": "2025-05-16T14:48:22.119141Z"
    }
   },
   "source": [
    "# Define the date categories\n",
    "date_categories = ['newest', 'newer', 'older', 'oldest']\n",
    "\n",
    "# Sort and assign date categories within each group\n",
    "df['date_category'] = (\n",
    "    df.sort_values(by='Created DateTime', ascending=False)\n",
    "    .groupby(['Model', 'Query'])\n",
    "    .cumcount()\n",
    "    .map({i: category for i, category in enumerate(date_categories)})\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.510254Z",
     "start_time": "2025-05-16T14:48:22.131648Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Model                  Query  Rank  Document ID    Created DateTime  \\\n",
       "0   gpt  What is test query 1?     1           33 2020-09-14 08:50:17   \n",
       "1   gpt  What is test query 1?     2           31 2024-06-18 15:35:22   \n",
       "2   gpt  What is test query 1?     3           30 2025-02-27 09:10:45   \n",
       "3   gpt  What is test query 1?     4         3985 2022-11-09 16:04:18   \n",
       "4   gpt  What is test query 2?     1         3985 2022-11-09 16:04:18   \n",
       "\n",
       "  date_category  \n",
       "0        oldest  \n",
       "1         newer  \n",
       "2        newest  \n",
       "3         older  \n",
       "4         older  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Query</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Created DateTime</th>\n",
       "      <th>date_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2020-09-14 08:50:17</td>\n",
       "      <td>oldest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-06-18 15:35:22</td>\n",
       "      <td>newer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>2025-02-27 09:10:45</td>\n",
       "      <td>newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 1?</td>\n",
       "      <td>4</td>\n",
       "      <td>3985</td>\n",
       "      <td>2022-11-09 16:04:18</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt</td>\n",
       "      <td>What is test query 2?</td>\n",
       "      <td>1</td>\n",
       "      <td>3985</td>\n",
       "      <td>2022-11-09 16:04:18</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:54:52.512358Z",
     "start_time": "2025-05-16T14:48:22.168421Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
